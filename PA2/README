README file for Programming Assignment 2 (C++ edition)
=====================================================

Your directory should now contain the following files:

 Makefile        -> [course dir]/src/PA2/Makefile
 README
 cool.flex
 test.cl
 lextest.cc      -> [course dir]/src/PA2/lextest.cc
 mycoolc         -> [course dir]/src/PA2/mycoolc
 stringtab.cc    -> [course dir]/src/PA2/stringtab.cc
 utilities.cc    -> [course dir]/src/PA2/utilities.cc
 handle_flags.cc -> [course dir]/src/PA2/handle_flags.cc
 *.d             dependency files
 *.*             other generated files

The include (.h) files for this assignment can be found in 
[course dir]/include/PA2

	The Makefile contains targets for compiling and running your
	program. DO NOT MODIFY.

	The README contains this info. Part of the assignment is to fill
	the README with the write-up for your project. You should
	explain design decisions, explain why your code is correct, and
	why your test cases are adequate. It is part of the assignment
	to clearly and concisely explain things in text as well as to
	comment your code. Just edit this file.

	cool.flex is a skeleton file for the specification of the
	lexical analyzer. You should complete it with your regular
	expressions, patterns and actions. Information on how to do this
	is in the flex manual, which is part of your reader.

	test.cl is a COOL program that you can test the lexical
	analyzer on. It contains some errors, so it won't compile with
	coolc. However, test.cl does not exercise all lexical
	constructs of COOL and part of your assignment is to rewrite
	test.cl with a complete set of tests for your lexical analyzer.

	cool-parse.h contains definitions that are used by almost all parts
	of the compiler. DO NOT MODIFY.

	stringtab.{cc|h} and stringtab_functions.h contains functions
        to manipulate the string tables.  DO NOT MODIFY.

	utilities.{cc|h} contains functions used by the main() part of
	the lextest program. You may want to use the strdup() function
	defined in here. Remember that you should not print anything
	from inside cool.flex! DO NOT MODIFY.

	lextest.cc contains the main function which will call your
	lexer and print out the tokens that it returns.  DO NOT MODIFY.

	mycoolc is a shell script that glues together the phases of the
	compiler using Unix pipes instead of statically linking code.  
	While inefficient, this architecture makes it easy to mix and match
	the components you write with those of the course compiler.
	DO NOT MODIFY.	

        cool-lexer.cc is the scanner generated by flex from cool.flex.
        DO NOT MODIFY IT, as your changes will be overritten the next
        time you run flex.

 	The *.d files are automatically generated Makefiles that capture
 	dependencies between source and header files in this directory.
 	These files are updated automatically by Makefile; see the gmake
 	documentation for a detailed explanation.

Instructions
------------

	To compile your lextest program type:

	% gmake lexer

	Run your lexer by putting your test input in a file 'foo.cl' and
	run the lextest program:

	% ./lexer foo.cl

	To run your lexer on the file test.cl type:

	% gmake dotest

	If you think your lexical analyzer is correct and behaves like
	the one we wrote, you can actually try 'mycoolc' and see whether
	it runs and produces correct code for any examples.
	If your lexical analyzer behaves in an
	unexpected manner, you may get errors anywhere, i.e. during
	parsing, during semantic analysis, during code generation or
	only when you run the produced code on spim. So beware.

	To turnin your work type:

	% gmake submit-clean

	And run the "submit" program following the instructions on the
	course web page.
	
	Running "submit" will collect the files cool.flex, test.cl,
	README, and test.output. Don't forget to edit the README file to
	include your write-up, and to write your own test cases in
	test.cl.

 	You may turn in the assignment as many times as you like.
	However, only the last version will be retained for
	grading.

	If you change architectures you must issue

	% gmake clean

	when you switch from one type of machine to the other.
	If at some point you get weird errors from the linker,	
	you probably forgot this step.

	GOOD LUCK!

---8<------8<------8<------8<---cut here---8<------8<------8<------8<---

Write-up for PA2
----------------

In this practical activity we implemented a lexical analyzer for the COOL language. 
We based our rules on the COOL manual, especially section 10, which describes the lexical structure of the language.

The identifiers can define an object and a type, both of which consist of a sequence of numbers, letters and underscores. The main difference between them is that the object must always start with a lowercase letter and the type with an uppercase letter. The basic types of the language, like Integer, String, and Object fall inside the type definition, the keyword 'self' also falls under it, even though it starts with a lowercase. 
The keywords in the COOL language are case insensitive, apart from 'true' and 'false', which must start with a lowercase letter. Therefore, our rules for them always starts with the option (?i:) which determines that the following pattern is case insensitive. 
The final definitions contain special characters, like blank spaces and escape characters, and the remainder operators, used for arithmetic, comparison, assignment, and others. 

Most of the actions are pretty much straightforward, when we find a token that matches one of the patterns created previously, we simply add its symbol to the stringtable and return the token that it matches, this is the case for the keywords, identifiers, numbers and operators. 
When finding a new line operator '\n' we simply increment the variable curr_lineno, which is used to keep track of the current line in the input file. 
When finding a whitespace, we don't do anything. 
And when finding anything else we raise an error, because whatever was found by the lexer shouldn't even be in the code in the first place. 

To parse comments and strings we take a different route, since they can span through multiple lines and can contain escape characters which need to be parsed. 
We utilize state conditions, which helps keep track of the current state of the lexer, and allows us to switch between them when needed, the rpogram allways starts in the INITIAl state.
For the comments we have two possible states, one for the block comment and one for the inline comment. 
When finding the pattern '--' we switch to the inline comment state, which may end with an end of line character '\n', an EOF, or the same pattern '--', returning to the INITIAL state. Everything else is ignored. 
When finding the pattern '(*' we switch to the block comment state, which must end with the pattern '*)', returning to the INITIAL state, finding an EOF results in an error, the rest is ignored. 
Also, when finding the '*)' pattern in the INITIAL state we raise the "Unmatched *)" error, since it means that a block comment was never opened. 

To parse strings, we find the " character, which indicates the start of one, and sends us to the parse_string state. 
In this state we add the characters in the string to our string buffer, whose position we track with the cursor variable, until we find the " character again, which indicates the end of the string, we then save the buffer string as a symbol, adding it to the string table and restarting the cursor before returning to the INITIAL state. 
It is necessary to parse the special ASCII characters, like \n, these characters themselves may NOT appear in the string because this will result in the error "Unterminated string constant", however they can be added to the buffer as long as they appear as two separated characters, like '\' and 'n', in this case they will be parsed as a single character and added to the buffer. 
In the event where the string is longer than 1024 characters we raise the "String constant too long" error, and switch to the state eat_string, which ignores the rest of the string until we find a new " character, sending the program back to the INITIAL state, if we find an EOF or an escaped character we raise a second error, but now for unterminated string constant. 

We conducted tests to each token and rule, covering every situation, while developing the lexer, the folder 'tests' contain some examples of the entries we used.
Besides that, we also tested the lexer in real COOL codes, we utilized the codes developed for TP1, which can also be found in the 'tests' folder, and for every example code available in the COOL directory "/var/tmp/cool/examples".

Students:
Rafael Ramos de Andrade
Victor Gabriel Mendes Sundermann
